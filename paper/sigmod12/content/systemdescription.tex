
\section{System Description}

We discuss the architecture, functionalities, and the implementation of
the {\system} system.
\begin{figure}
   \begin{center}
        \includegraphics[scale=0.2]{content/graphics/arch.png}
        \caption{{\system} architecture}
        \label{fig:arch}
   \end{center}
\end{figure}
    
\subsection{System Architecture}

{\system} is a four layered system, as can be seen in Figure \ref{fig:arch}.
The user interface is where both naive and advanced users can construct queries over
text, structured data, and models. From the user interface, queries are then passed to the
DBMS, where MADLib and {\system} libraries sit on top of the query processor to
add statistical and text processing functionality. These queries are 
processed using PostgreSQL/Greenplum's Parallel DB architecture to further
optimize on replicated storage. 


\subsection{{\system}: Statistical Text Analysis Tasks}
{\system} focuses on four different text analysis tasks: 
information extraction, entity resolution,
sentiment analysis, and part-of-speech (POS) tagging.\\

\noindent
\textbf{Information Extraction}\\
Once we have the text documents, we would like to then be able to identify 
objects of interest. Information Extraction methods become increasingly
important as the size of our corpus gets large. Many methods exist for in-database data
extraction, but we are currently focused on building upon existing work in 
utilizing Conditional Random Fields (CRFs) for query-time extraction of 
entities\cite{wang2011hybrid}.\\

\noindent
\textbf{Entity Resolution}\\
Certain entities may be represented by
different names, due to misspellings in the text, or aliases. It is important to
resolve these entities appropriately to better understand the data. Increasingly
informal text, like blog posts and tweets, will require entity resolution. We
use q-grams to aid in the process. 
\\

\noindent
\textbf{Sentiment Analysis}
Sentiment analysis deals with the study of opinions expressed within text. 
The sentiment is usually expressed in terms of polarities, where positive
sentiment refers to the positive nature of the expressed opinion, and negative
nature for negative sentiment. Much work has already been done in this
area for document-level and entity-level sentiment \cite{zhang2011combining, 
o2010tweets}. Our goal is to apply these existing techniques for use in 
native SQL queries. The current {\system} system focused only on extracting
document-level sentiment, but we expect to extend the sentiment functionality to 
entity-level sentiment for more interesting queries.\\

\noindent
\textbf{Part-of-Speech Tagging}\\

Core to many natural language processing(NLP) tasks, POS involves the labeling of
terms within text based on their function in a particular sentence. MADden uses our
own implementation of POS in Postgres and Greenplum. We are committing our code to
MADLib and it is in under review. The advantage of our implementation lies in two
aspects. The first one is that we extract features for each distinct tokens instead of
for every tokens.  It is the easy to imagine the benefit it can bring when we are
labeling a large corpus since the number of distinct tokens is limited to the size
of English vocabulary. We have a function 'MADLIB\_SCHEMA.textfex\_generatemrtbl' to
generate all the features. This function produces two factors tables: a 'rfactor'
table, which store single state features, e.g., 'RegexFeature' and a 'mfactor'
table to store the two state features such as 'EdgeFeature'.  The second advantage is
that we use SQL statements as the control flow driver which is inherently
parallel for POS tagging. And the computation intensive functions are implemented in C. The
following SQL statements is for geting the best POS label sequence for all sentences
stored in segtbl.  For each sentence, 'MADLIB\_SCHEMA.\_\_vcrf\_top1\_label' implemented in
C is invoked to get the best label sequence.

INSERT INTO resulttbl

SELECT   segtbl.doc\_id, MADLIB\_SCHEMA.\_\_vcrf\_top1\_label(segtbl.seglist, 
         mfactor.score, rfactor.score) 

FROM   segtbl, mfactor, rfactor;

Core to many natural language processing (NLP) tasks, POS tagging
involves the labeling of terms within text based on their function in a
particular sentence. For the current implementation of {\system}, we are 
utilizing existing packages, such as those found in the Natural Language 
Toolkit (NLTK~\footnote{http://www.nltk.org/}).

%\subsection{In-Database Statistical Methods}
\subsection{{\system} Implementation}

% TODO develop this section more!!

\system algorithms are developed to perform optimally for the data format.
The algorithms are first implemented in SQL or PL/pgSQL, as a script and 
exposes the execution pipeline to the query optimizer. 
Hybrid query optimization techniques to achieve query speed up is discussed in
our previous work \cite{wang2011hybrid}.
SQL excels in batch relation and group operations, we use Python language
scripts as a glue language to manage to execution of several complicated 
queries and to control the data flow especially loops.

% MADlib provides a backbone for the text analytics functionality, including
% Naive Bayes, LDA \cite{Blei:2003:LDA:944919.944937}, and Sparse Vector
% functions. MADLib also employs CRF inference, using Viterbi and
%MCMC methods that can be used for information extraction 
%\cite{wang2011hybrid}. The CRF inference methods, like the others methods, are
%all implemented in native SQL.

To enable text analytics, {\system} works alongside statistical
functions found in the MADlib library \cite{Cohen:2009:MSN:1687553.1687576}.

Dense array and matrix operations are efficiently performed 
through calls to highly tuned numeric packages such as LAPACK.

%\subsection{{\system} Implementation}
%The implementation of {\system} utilizes these key features to process 
%text analysis queries.\\
%We discuss the 

\noindent
\textbf{Data Parallelism}\\
With the ability to implement a parallel database architecture with Postgres or 
Greenplum, we can utilize data parallelism to further optimize queries written
with {\system}. Each node within the parallel DB could run some query
over a subset of the data (data parallel). This includes the statistical methods
in MADLib, which were all built to be data parallel.

\noindent
\textbf{Query Processing and Optimization} \\
Since the text analytics are being done as part of our SQL query, and because
these methods are implemented in SQL, they are subject to the same optimizations
that normal SQL queries would. \\


\noindent
\textbf{Indices}\\
{\system} utilizes inverted indices within the database to perform text
analysis on documents. We can scan the inverted index of each 
document, filtering
out documents that do not contain instances of the player names.
For simple matching, we may use the exact names of the players. But to
deal with players having multiple names within our corpus (e.g. misspellings,
nicknames), we can perform entity
resolution using alias tables, as well as methods such as q-grams in the DBMS to perform fuzzy string
matching \cite{Jain:2009:BQO:1519103.1519108}.\\

\noindent
\textbf{Views of Text Analysis Results}\\
Each text analysis function creates a view that can continue to be utilized.
Since views can be utilized in the same manner as tables, we can join calculated
statistics with other tables, or other text analysis views, to construct more
interesting results. This, combined with expressing text analysis functionality
within a SQL query, allows for more complex queries to be realized. \\



